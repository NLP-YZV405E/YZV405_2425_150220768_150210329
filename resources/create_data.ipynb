{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITU Data to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>expression</th>\n",
       "      <th>category</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr</td>\n",
       "      <td>Ödevler, Sınavlar, Evet yaşanması gereken zama...</td>\n",
       "      <td>['Ödevler', ',', 'Sınavlar', ',', 'Evet', 'yaş...</td>\n",
       "      <td>zaman öldürmek</td>\n",
       "      <td>idiomatic</td>\n",
       "      <td>[7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr</td>\n",
       "      <td>Belediye bugün yolları açtı.</td>\n",
       "      <td>['Belediye', 'bugün', 'yolları', 'açtı', '.']</td>\n",
       "      <td>yol açmak</td>\n",
       "      <td>literal</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr</td>\n",
       "      <td>Planı ortaya çıkmasın diye elinden geleni yapmış.</td>\n",
       "      <td>['Planı', 'ortaya', 'çıkmasın', 'diye', 'elind...</td>\n",
       "      <td>ortaya çıkmak</td>\n",
       "      <td>idiomatic</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tr</td>\n",
       "      <td>Aldığın bunca tedbire rağmen halen üstüne geli...</td>\n",
       "      <td>['Aldığın', 'bunca', 'tedbire', 'rağmen', 'hal...</td>\n",
       "      <td>üstüne almak</td>\n",
       "      <td>literal</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tr</td>\n",
       "      <td>Yazdıkların yanlış olmuş, istersen defterden s...</td>\n",
       "      <td>['Yazdıkların', 'yanlış', 'olmuş', ',', 'ister...</td>\n",
       "      <td>defterden silmek</td>\n",
       "      <td>literal</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language                                           sentence  \\\n",
       "id                                                               \n",
       "1        tr  Ödevler, Sınavlar, Evet yaşanması gereken zama...   \n",
       "2        tr                       Belediye bugün yolları açtı.   \n",
       "3        tr  Planı ortaya çıkmasın diye elinden geleni yapmış.   \n",
       "4        tr  Aldığın bunca tedbire rağmen halen üstüne geli...   \n",
       "5        tr  Yazdıkların yanlış olmuş, istersen defterden s...   \n",
       "\n",
       "                                   tokenized_sentence        expression  \\\n",
       "id                                                                        \n",
       "1   ['Ödevler', ',', 'Sınavlar', ',', 'Evet', 'yaş...    zaman öldürmek   \n",
       "2       ['Belediye', 'bugün', 'yolları', 'açtı', '.']         yol açmak   \n",
       "3   ['Planı', 'ortaya', 'çıkmasın', 'diye', 'elind...     ortaya çıkmak   \n",
       "4   ['Aldığın', 'bunca', 'tedbire', 'rağmen', 'hal...      üstüne almak   \n",
       "5   ['Yazdıkların', 'yanlış', 'olmuş', ',', 'ister...  defterden silmek   \n",
       "\n",
       "     category indices  \n",
       "id                     \n",
       "1   idiomatic  [7, 8]  \n",
       "2     literal    [-1]  \n",
       "3   idiomatic  [1, 2]  \n",
       "4     literal    [-1]  \n",
       "5     literal    [-1]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itu_df = pd.read_csv(r\"../data/public_data/train.csv\", index_col=\"id\")\n",
    "itu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"../data/public_data/train.csv\"\n",
    "dev_path = r\"../data/public_data/eval.csv\"\n",
    "test_path = r\"../data/public_data/test.csv\"\n",
    "\n",
    "itu_to_tsv(train_path, r\"./ITU/train.tsv\")\n",
    "itu_to_tsv(dev_path, r\"./ITU/dev.tsv\")\n",
    "\n",
    "itu_to_tsv_test(test_path, r\"./ITU/test.tsv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSAME Data to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Türkiye', False)\n",
      "(\"'de\", False)\n",
      "('bankaların', False)\n",
      "('bir', False)\n",
      "('siyasi', False)\n",
      "('gücü', False)\n",
      "('de', False)\n",
      "('var', False)\n",
      "(',', False)\n",
      "('bu', False)\n",
      "('nedenle', False)\n",
      "('ilerde', False)\n",
      "('_', False)\n",
      "('örgütlenmeye', False)\n",
      "('de', False)\n",
      "('yardımcı', False)\n",
      "('olabilirler', False)\n",
      "('.', False)\n"
     ]
    }
   ],
   "source": [
    "cupt_data = parse_cupt(r\"../data/PARSAME 1.1 data/sharedtask-1.1-TR/train.cupt\")\n",
    "for word in cupt_data[1]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output folders exist\n",
    "tr_out_path = r\"../data/PARSAME 1.1 data/transformed-TR/\"\n",
    "it_out_path = r\"../data/PARSAME 1.1 data/transformed-IT/\"\n",
    "os.makedirs(tr_out_path, exist_ok=True)\n",
    "os.makedirs(it_out_path, exist_ok=True)\n",
    "\n",
    "tr_in_path = r\"../data/PARSAME 1.1 data/sharedtask-1.1-TR/\"\n",
    "it_in_path = r\"../data/PARSAME 1.1 data/sharedtask-1.1-IT/\"\n",
    "\n",
    "# Transform Turkish\n",
    "transform_cupt_to_tsv(tr_in_path+\"train.cupt\", tr_out_path+\"train.tsv\", \"tr\")\n",
    "transform_cupt_to_tsv(tr_in_path+\"dev.cupt\", tr_out_path+\"dev.tsv\", \"tr\")\n",
    "transform_cupt_to_tsv(tr_in_path+\"test.cupt\", tr_out_path+\"test.tsv\", \"tr\")\n",
    "\n",
    "# Transform Italian\n",
    "transform_cupt_to_tsv(it_in_path+\"train.cupt\", it_out_path+\"train.tsv\", \"it\")\n",
    "transform_cupt_to_tsv(it_in_path+\"dev.cupt\", it_out_path+\"dev.tsv\", \"it\")\n",
    "transform_cupt_to_tsv(it_in_path+\"test.cupt\", it_out_path+\"test.tsv\", \"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "parsame_output_path = r\"./PARSAME/\"\n",
    "os.makedirs(parsame_output_path, exist_ok=True)\n",
    "\n",
    "# Combine train, dev, test TSVs\n",
    "combine_tsv_files(tr_out_path+\"train.tsv\", it_out_path+\"train.tsv\", parsame_output_path+\"train.tsv\")\n",
    "combine_tsv_files(tr_out_path+\"dev.tsv\", it_out_path+\"dev.tsv\", parsame_output_path+\"dev.tsv\")\n",
    "combine_tsv_files(tr_out_path+\"test.tsv\", it_out_path+\"test.tsv\", parsame_output_path+\"test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID10M\n",
    "\n",
    "ID10M is alredy in tsv format we need to add language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_input_path = r\"../data/ID10M/\"\n",
    "# check outpath\n",
    "out_path = r\"./ID10M/\"\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "# Add language to ID10M files\n",
    "add_language(id_input_path+\"train.tsv\", out_path+\"train.tsv\", \"it\")\n",
    "add_language(id_input_path+\"dev.tsv\", out_path+\"dev.tsv\", \"it\")\n",
    "add_language(id_input_path+\"test.tsv\", out_path+\"test.tsv\", \"it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 778458",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m combine_tsv_files(parsame_output_path+\u001b[33m\"\u001b[39m\u001b[33mtrain.tsv\u001b[39m\u001b[33m\"\u001b[39m, out_path+\u001b[33m\"\u001b[39m\u001b[33mtrain.tsv\u001b[39m\u001b[33m\"\u001b[39m, combined_output_path+\u001b[33m\"\u001b[39m\u001b[33mtrain.tsv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load the combined dataset to verify\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m combined_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_output_path\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain.tsv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(combined_df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krbyk\\miniconda3\\envs\\octa\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krbyk\\miniconda3\\envs\\octa\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krbyk\\miniconda3\\envs\\octa\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krbyk\\miniconda3\\envs\\octa\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: EOF inside string starting at row 778458"
     ]
    }
   ],
   "source": [
    "# Create output folder for combined dataset\n",
    "combined_output_path = r\"./COMBINED/\"\n",
    "\n",
    "os.makedirs(combined_output_path,exist_ok=True)\n",
    "\n",
    "# Combine all train files into a single combined.tsv file\n",
    "combine_all_tsv_files()\n",
    "\n",
    "# Load the combined dataset to verify\n",
    "combined_df = pd.read_csv(combined_output_path+\"train.tsv\", sep=\"\\t\", index_col=\"id\")\n",
    "print(combined_df.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
