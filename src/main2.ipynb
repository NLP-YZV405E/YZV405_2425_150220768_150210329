{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berkekurt/anaconda3/envs/nlpenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/berkekurt/Documents/GitHub/YZV405_2425_150220768_150210329/src/main2.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berkekurt/Documents/GitHub/YZV405_2425_150220768_150210329/src/main2.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m tr_config \u001b[39m=\u001b[39m BertConfig\u001b[39m.\u001b[39mfrom_pretrained(tr_model_name, output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berkekurt/Documents/GitHub/YZV405_2425_150220768_150210329/src/main2.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m tr_tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tr_model_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/berkekurt/Documents/GitHub/YZV405_2425_150220768_150210329/src/main2.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m hf_tr_model \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39;49mfrom_pretrained(tr_model_name, config\u001b[39m=\u001b[39;49mtr_config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berkekurt/Documents/GitHub/YZV405_2425_150220768_150210329/src/main2.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# train, update or test mode selection\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berkekurt/Documents/GitHub/YZV405_2425_150220768_150210329/src/main2.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#mode = input(\"Do you want to train or test the model? (train, update, test): \").strip().lower()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/berkekurt/Documents/GitHub/YZV405_2425_150220768_150210329/src/main2.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[39m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/transformers/modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4250\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4251\u001b[0m     gguf_file\n\u001b[1;32m   4252\u001b[0m     \u001b[39mand\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[39mand\u001b[39;00m ((\u001b[39misinstance\u001b[39m(device_map, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map\u001b[39m.\u001b[39mvalues()) \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m device_map)\n\u001b[1;32m   4254\u001b[0m ):\n\u001b[1;32m   4255\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4256\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4257\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloaded from GGUF files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m     )\n\u001b[0;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[39m=\u001b[39m _get_resolved_checkpoint_files(\n\u001b[1;32m   4261\u001b[0m     pretrained_model_name_or_path\u001b[39m=\u001b[39;49mpretrained_model_name_or_path,\n\u001b[1;32m   4262\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   4263\u001b[0m     variant\u001b[39m=\u001b[39;49mvariant,\n\u001b[1;32m   4264\u001b[0m     gguf_file\u001b[39m=\u001b[39;49mgguf_file,\n\u001b[1;32m   4265\u001b[0m     from_tf\u001b[39m=\u001b[39;49mfrom_tf,\n\u001b[1;32m   4266\u001b[0m     from_flax\u001b[39m=\u001b[39;49mfrom_flax,\n\u001b[1;32m   4267\u001b[0m     use_safetensors\u001b[39m=\u001b[39;49muse_safetensors,\n\u001b[1;32m   4268\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   4269\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   4270\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   4271\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   4272\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   4273\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   4274\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   4275\u001b[0m     commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   4276\u001b[0m )\n\u001b[1;32m   4278\u001b[0m is_sharded \u001b[39m=\u001b[39m sharded_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m is_quantized \u001b[39m=\u001b[39m hf_quantizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/transformers/modeling_utils.py:997\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    985\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[1;32m    986\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[1;32m    996\u001b[0m     }\n\u001b[0;32m--> 997\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[1;32m    999\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   1002\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[39m=\u001b[39m cached_files(path_or_repo_id\u001b[39m=\u001b[39;49mpath_or_repo_id, filenames\u001b[39m=\u001b[39;49m[filename], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    267\u001b[0m     file \u001b[39m=\u001b[39m file[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[39mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(full_filenames) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m         \u001b[39m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m         hf_hub_download(\n\u001b[1;32m    425\u001b[0m             path_or_repo_id,\n\u001b[1;32m    426\u001b[0m             filenames[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    427\u001b[0m             subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    428\u001b[0m             repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    429\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    430\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    431\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    432\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    433\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    434\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    435\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    436\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    437\u001b[0m         )\n\u001b[1;32m    438\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m         snapshot_download(\n\u001b[1;32m    440\u001b[0m             path_or_repo_id,\n\u001b[1;32m    441\u001b[0m             allow_patterns\u001b[39m=\u001b[39mfull_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m             local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    451\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    942\u001b[0m         \u001b[39m# Destination\u001b[39;00m\n\u001b[1;32m    943\u001b[0m         local_dir\u001b[39m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    959\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m    962\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    964\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m    966\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    967\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    968\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    969\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m    971\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m    972\u001b[0m         headers\u001b[39m=\u001b[39;49mhf_headers,\n\u001b[1;32m    973\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    974\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    975\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    977\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    978\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1112\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1110\u001b[0m Path(lock_path)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1111\u001b[0m \u001b[39mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1112\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1113\u001b[0m         incomplete_path\u001b[39m=\u001b[39;49mPath(blob_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.incomplete\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1114\u001b[0m         destination_path\u001b[39m=\u001b[39;49mPath(blob_path),\n\u001b[1;32m   1115\u001b[0m         url_to_download\u001b[39m=\u001b[39;49murl_to_download,\n\u001b[1;32m   1116\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1117\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1118\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1119\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1120\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1121\u001b[0m         etag\u001b[39m=\u001b[39;49metag,\n\u001b[1;32m   1122\u001b[0m         xet_file_data\u001b[39m=\u001b[39;49mxet_file_data,\n\u001b[1;32m   1123\u001b[0m     )\n\u001b[1;32m   1124\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1125\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1675\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[39mif\u001b[39;00m xet_file_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1670\u001b[0m             logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1671\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mXet Storage is enabled for this repo, but the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhf_xet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m package is not installed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1672\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFalling back to regular HTTP download. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1673\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1674\u001b[0m             )\n\u001b[0;32m-> 1675\u001b[0m         http_get(\n\u001b[1;32m   1676\u001b[0m             url_to_download,\n\u001b[1;32m   1677\u001b[0m             f,\n\u001b[1;32m   1678\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1679\u001b[0m             resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1680\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1681\u001b[0m             expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1682\u001b[0m         )\n\u001b[1;32m   1684\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownload complete. Moving file to \u001b[39m\u001b[39m{\u001b[39;00mdestination_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1685\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:449\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    447\u001b[0m new_resume_size \u001b[39m=\u001b[39m resume_size\n\u001b[1;32m    448\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mconstants\u001b[39m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    450\u001b[0m         \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    451\u001b[0m             progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m   1068\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt, read1\u001b[39m=\u001b[39;49mread1) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __init__ import *\n",
    "from dataset import IdiomDataset\n",
    "from collate import collate\n",
    "from model import IdiomExtractor\n",
    "from bert_embedder import BERTEmbedder\n",
    "from hparams import HParams\n",
    "from trainer import Trainer\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "SEED = 2\n",
    "# set seeds to get reproducible results\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "# gpuda bazen randomluk olabiliyormuş onu kaldırmak için\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# create bert\n",
    "it_model_name = 'bert-base-multilingual-cased'\n",
    "# output hidden states -> it helps to get hidden states from bert\n",
    "it_config = BertConfig.from_pretrained(it_model_name, output_hidden_states=True)\n",
    "it_tokenizer = BertTokenizer.from_pretrained(it_model_name)\n",
    "# get bert weights\n",
    "hf_it_model = BertModel.from_pretrained(it_model_name, config=it_config)\n",
    "\n",
    "\n",
    "# Türkçe BERT\n",
    "tr_model_name = \"dbmdz/bert-base-turkish-128k-cased\"\n",
    "tr_config = BertConfig.from_pretrained(tr_model_name, output_hidden_states=True)\n",
    "tr_tokenizer = BertTokenizer.from_pretrained(tr_model_name)\n",
    "hf_tr_model = BertModel.from_pretrained(tr_model_name, config=tr_config)\n",
    "\n",
    "# train, update or test mode selection\n",
    "#mode = input(\"Do you want to train or test the model? (train, update, test): \").strip().lower()\n",
    "mode = \"train\"\n",
    "assert mode in ['train', 'update', 'test'], \"Mode must be one of train, update, test\"\n",
    "# select the dataset\n",
    "#dataset_selection = input(\"Select the dataset (ID10M, ITU, PARSEME, ALL_COMBINED): \").strip().upper()\n",
    "dataset_selection = \"ITU\"\n",
    "assert dataset_selection in ['ID10M', 'ITU', 'PARSEME', 'COMBINED'], \"Dataset must be one of ID10M, ITU, PARSEME, COMBINED\"\n",
    "\n",
    "# check dataset path\n",
    "tr_path = r\"./src/checkpoints/tr/\"\n",
    "it_path = r\"./src/checkpoints/it/\"\n",
    "os.makedirs(tr_path, exist_ok=True)\n",
    "os.makedirs(it_path, exist_ok=True)\n",
    "\n",
    "if mode in [\"test\",\"update\"]:\n",
    "    # list available checkpoints\n",
    "    print(\"Available tr checkpoints:\")\n",
    "    checkpoints = os.listdir(tr_path)\n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        print(f\"{i+1}. {checkpoint}\")\n",
    "    print(\"none\")\n",
    "    # load the model\n",
    "    checkpoint = input(\"Enter the checkpoint (without .pt): \").strip()\n",
    "    if checkpoint == \"none\":\n",
    "        tr_path = None\n",
    "    else:\n",
    "        tr_path = tr_path + checkpoint + \".pt\"\n",
    "        assert os.path.exists(tr_path), \"Model path does not exist\"\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Available it checkpoints:\")\n",
    "    checkpoints = os.listdir(it_path)\n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        print(f\"{i+1}. {checkpoint}\")\n",
    "    print(\"none\")\n",
    "    # load the model\n",
    "    checkpoint = input(\"Enter the checkpoint (without .pt): \").strip()\n",
    "    if checkpoint == \"none\":\n",
    "        it_path = None\n",
    "    else:\n",
    "        it_path = it_path + checkpoint + \".pt\"\n",
    "        assert os.path.exists(it_path), \"Model path does not exist\"\n",
    "\n",
    "model_name = None\n",
    "if mode in [\"train\", \"update\"]:\n",
    "    #model_name = input(\"Enter the model name (without .pt): \").strip()\n",
    "    model_name = \"deneme\"\n",
    "\n",
    "elif mode == \"test\":\n",
    "    model_name = checkpoint\n",
    "\n",
    "# get stanza tagger for both languages\n",
    "tagger_dict = initialize(use_gpu=True)\n",
    "\n",
    "# get the path for the dataset\n",
    "main_path = r\"../resources/\"+dataset_selection+\"/\"\n",
    "train_file = main_path + \"train.tsv\"\n",
    "dev_file = main_path + \"dev.tsv\"\n",
    "test_file = main_path + \"test.tsv\"\n",
    "\n",
    "labels_vocab = {\"<pad>\":0, \"B-IDIOM\":1, \"I-IDIOM\":2, \"O\":3}\n",
    "\n",
    "# initialize the dataset\n",
    "train_dataset, dev_dataset, test_dataset = None, None, None\n",
    "if mode in [\"train\", \"update\"]:\n",
    "    train_dataset = IdiomDataset(train_file, labels_vocab, tagger_dict)\n",
    "    dev_dataset = IdiomDataset(dev_file, labels_vocab, tagger_dict)\n",
    "    print(f\"train sentences: {len(train_dataset)}\")\n",
    "    print(f\"dev sentences: {len(dev_dataset)}\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "else:\n",
    "    test_dataset = IdiomDataset(test_file, labels_vocab, tagger_dict) \n",
    "    print(f\"test sentences: {len(test_dataset)}\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "#dataloader\n",
    "\n",
    "if mode in [\"train\", \"update\"]:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=16, collate_fn=collate)\n",
    "    print(f\"length of train dataloader: {len(train_dataloader)}\")\n",
    "    print(f\"length of dev dataloader: {len(dev_dataloader)}\")\n",
    "else:\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=collate)\n",
    "    print(f\"length of test dataloader: {len(test_dataloader)}\")\n",
    "\n",
    "\n",
    "#instantiate the hyperparameters\n",
    "params = HParams()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#instantiate the model\n",
    "it_model = IdiomExtractor(hf_it_model,\n",
    "                    params).cuda()\n",
    "\n",
    "it_model.freeze_bert()\n",
    "\n",
    "tr_model = IdiomExtractor(hf_tr_model,\n",
    "                    params).cuda()\n",
    "\n",
    "tr_model.freeze_bert()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "it_embedder =  BERTEmbedder(hf_it_model, it_tokenizer, device)\n",
    "tr_embedder =  BERTEmbedder(hf_tr_model, tr_tokenizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of words: 16, example: ['Zaman', 'kazanmak', 'için', 'yaptığın', 'entrikalar', 'seni', 'kurtarmayacak', ',', 'eninde', 'sonunda', 'yakalayacak', 'seni', 'polis', '!']\n",
      "shape of labels: 16, example: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')\n",
      "shape of langs: 16, example: tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "words, labels, langs = example_sentence\n",
    "\n",
    "# embedding'i aynı size a getirmek için lazım\n",
    "# labellar beraber padleniyor embeddingler ayrı\n",
    "# yani en uzun tr cümle 14, it cümle 12 uzunluğundaysa\n",
    "# it label 14, it embedding 12 boyutunda oluyor hata veriyor.\n",
    "global_max = labels.size(1)\n",
    "\n",
    "print(f\"shape of words: {len(words)}, example: {words[0]}\")\n",
    "print(f\"shape of labels: {len(labels)}, example: {labels[0]}\")\n",
    "print(f\"shape of langs: {len(langs)}, example: {langs}\")\n",
    "print(global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_indices: tensor([ 0,  1,  5,  7, 10, 11, 12], device='cuda:0')\n",
      "it_indices: tensor([ 2,  3,  4,  6,  8,  9, 13, 14, 15], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tr_indices = (langs == 0).nonzero(as_tuple=True)[0]\n",
    "it_indices = (langs == 1).nonzero(as_tuple=True)[0]\n",
    "print(f\"tr_indices: {tr_indices}\")\n",
    "print(f\"it_indices: {it_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0],\n",
      "        [1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0],\n",
      "        [2, 3, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 1, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0],\n",
      "        [3, 3, 3, 1, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 2, 0, 0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of tr_words: 7, example: ['Zaman', 'kazanmak', 'için', 'yaptığın', 'entrikalar', 'seni', 'kurtarmayacak', ',', 'eninde', 'sonunda', 'yakalayacak', 'seni', 'polis', '!']\n",
      "shape of tr_labels: torch.Size([7, 14]), example: tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0],\n",
      "        [2, 3, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tr_words = [words[i] for i in tr_indices.cpu().numpy()]\n",
    "tr_labels = labels[tr_indices] # 0 1 5 7 10 11 12. labellar tr dilinin labelları\n",
    "\n",
    "print(f\"length of tr_words: {len(tr_words)}, example: {tr_words[0]}\")\n",
    "print(f\"shape of tr_labels: {tr_labels.shape}, example: {tr_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embedded_tr: 7, example: torch.Size([14, 768])\n",
      "shape of embedded_tr: 7, example: torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "# embed tr list of embeddings herbirisi 10,seq_length,768 -> [tr_batch_size, seq_length, 768]\n",
    "tr_embedded = tr_embedder.embed_sentences(tr_words)\n",
    "print(f\"shape of embedded_tr: {len(tr_embedded)}, example: {tr_embedded[0].shape}\")\n",
    "print(f\"shape of embedded_tr: {len(tr_embedded)}, example: {tr_embedded[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tr_embs: torch.Size([7, 14, 768]), example: tensor([[ 0.3040, -1.6375,  0.9913,  ...,  0.6738,  1.7467,  0.5538],\n",
      "        [-0.4647,  0.5055, -0.9381,  ...,  3.0271,  3.8738, -0.3686],\n",
      "        [ 3.6571, -1.3875, -0.9558,  ...,  1.5002,  2.6590,  1.7305],\n",
      "        ...,\n",
      "        [-2.0102, -1.6403, -0.8219,  ..., -1.0591,  1.9849, -4.5420],\n",
      "        [-0.4976, -2.3514,  1.0389,  ...,  1.2063,  2.7370, -4.2313],\n",
      "        [ 0.6042, -0.8540,  0.2265,  ..., -1.0613,  1.4576, -5.0178]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# tr embedded'i -1le padleyerek seq_lengthi eşitliyoruz. -> [tr_batch_size, max_seq_length, 768]\n",
    "tr_embs = pad_sequence(tr_embedded, batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "# check embedding size to match the labels\n",
    "if tr_embs.size(1) < global_max:\n",
    "    # ekstra pad lazım\n",
    "    pad_size = global_max - tr_embs.size(1)\n",
    "\n",
    "    tr_embs = F.pad(tr_embs, (0, 0, 0, pad_size), \"constant\", 0)\n",
    "\n",
    "\n",
    "print(f\"shape of tr_embs: {tr_embs.shape}, example: {tr_embs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of it_labels: torch.Size([9, 14]), example: tensor([[1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0],\n",
      "        [3, 1, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0],\n",
      "        [3, 3, 3, 1, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 2, 0, 0, 0]], device='cuda:0')\n",
      "shape of it_words: 9, example: ['Prendi', 'con', 'le', 'pinze', 'quello', 'che', 'non', 'è', 'detto', 'da', 'esperti']\n"
     ]
    }
   ],
   "source": [
    "it_words = [words[i] for i in it_indices.cpu().numpy()]\n",
    "it_labels = labels[it_indices]\n",
    "\n",
    "print(f\"shape of it_labels: {it_labels.shape}, example: {it_labels}\")\n",
    "print(f\"shape of it_words: {len(it_words)}, example: {it_words[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embedded_it: 9, example: torch.Size([11, 768])\n",
      "shape of embedded_it: 9, example: torch.Size([9, 768])\n"
     ]
    }
   ],
   "source": [
    "# embed tr list of embeddings herbirisi 10,seq_length,768 -> [tr_batch_size, seq_length, 768]\n",
    "it_embedded = it_embedder.embed_sentences(it_words)\n",
    "print(f\"shape of embedded_it: {len(it_embedded)}, example: {it_embedded[0].shape}\")\n",
    "print(f\"shape of embedded_it: {len(it_embedded)}, example: {it_embedded[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of it_embs: torch.Size([9, 14, 768]), example: tensor([[ 1.9155,  0.1461, -1.2802,  ...,  0.8907,  0.8552,  0.4204],\n",
      "        [ 1.1491,  3.5726,  0.6087,  ..., -0.1244,  2.5063,  1.2722],\n",
      "        [ 0.5323, -0.2845,  0.3650,  ..., -2.2384,  2.5075,  1.3367],\n",
      "        ...,\n",
      "        [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "it_embs = pad_sequence(it_embedded, batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "# check embedding size to match the labels\n",
    "if it_embs.size(1) < global_max:\n",
    "    # ekstra pad lazım\n",
    "    pad_size = global_max - it_embs.size(1)\n",
    "\n",
    "    it_embs = F.pad(it_embs, (0, 0, 0, pad_size), \"constant\", 0)\n",
    "\n",
    "print(f\"shape of it_embs: {it_embs.shape}, example: {it_embs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_NLL: 14.678117752075195\n",
      "it_NLL: 17.433109283447266\n",
      "loss: 32.111228942871094\n"
     ]
    }
   ],
   "source": [
    "tr_LL, _ = tr_model(tr_embs, tr_labels)\n",
    "tr_NLL = -tr_LL.sum() / len(tr_indices)\n",
    "\n",
    "it_LL,_ = it_model(it_embs, it_labels)\n",
    "it_NLL = -it_LL.sum() / len(it_indices)\n",
    "\n",
    "print(f\"tr_NLL: {tr_NLL}\")\n",
    "print(f\"it_NLL: {it_NLL}\")\n",
    "loss = tr_NLL + it_NLL\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "tr_optimizer = optim.Adam(tr_model.parameters(), lr=0.0001)\n",
    "it_optimizer = optim.Adam(it_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Optimizer step\n",
    "tr_optimizer.zero_grad()\n",
    "it_optimizer.zero_grad()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(tr_model.parameters(), 1)\n",
    "torch.nn.utils.clip_grad_norm_(it_model.parameters(), 1)\n",
    "tr_optimizer.step()\n",
    "it_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token‐seviye accuracy: 0.3403\n",
      "Sentence‐seviye accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# get one batch from dev set\n",
    "eval_data = next(iter(dev_dataloader))\n",
    "words, labels, langs = eval_data\n",
    "\n",
    "# tr ve it nin indexlerini ayıkla\n",
    "tr_indices = (langs == 0).nonzero(as_tuple=True)[0]\n",
    "it_indices = (langs == 1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "batch_size, seq_len = labels.shape\n",
    "device = labels.device\n",
    "global_max = seq_len\n",
    "\n",
    "hidden_size = tr_embedder.bert_model.config.hidden_size\n",
    "\n",
    "# TR cümleleri ayıkla, embeddle ve petle\n",
    "tr_words  = [words[i] for i in tr_indices.cpu().numpy()]\n",
    "tr_labels = labels[tr_indices]\n",
    "# türkçe cümle gelmiş mi kontrolü\n",
    "if len(tr_words) > 0:\n",
    "    tr_embedded = tr_embedder.embed_sentences(tr_words)\n",
    "    tr_embs     = pad_sequence(tr_embedded, batch_first=True, padding_value=0).to(device)\n",
    "    if tr_embs.size(1) < global_max:\n",
    "        # ekstra pad lazım\n",
    "        pad_size = global_max - tr_embs.size(1)\n",
    "        tr_embs  = F.pad(tr_embs, (0, 0, 0, pad_size), \"constant\", 0)\n",
    "else:\n",
    "    # hiç TR cümlesi yoksa boş batch oluştur\n",
    "    tr_embs = torch.zeros((0, global_max, hidden_size), device=device)\n",
    "\n",
    "# IT cümleleri ayıkla, embeddle ve petle\n",
    "it_words    = [words[i] for i in it_indices.cpu().numpy()]\n",
    "it_labels   = labels[it_indices]\n",
    "if len(it_words) > 0:\n",
    "    it_embedded = it_embedder.embed_sentences(it_words)\n",
    "    it_embs     = pad_sequence(it_embedded, batch_first=True, padding_value=0).to(device)\n",
    "    if it_embs.size(1) < global_max:\n",
    "        # ekstra pad lazım\n",
    "        pad_size = global_max - it_embs.size(1)\n",
    "        it_embs  = F.pad(it_embs, (0, 0, 0, pad_size), \"constant\", 0)\n",
    "else:\n",
    "    # hiç IT cümlesi yoksa boş batch oluştur\n",
    "    it_embs = torch.zeros((0, global_max, hidden_size), device=device)\n",
    "\n",
    "# take predictions\n",
    "# inference: forward(…, None) dönen list of lists\n",
    "tr_decode = tr_model(tr_embs, None)\n",
    "it_decode = it_model(it_embs, None)\n",
    "\n",
    "# alt kısım GPT bakmak lazım \n",
    "\n",
    "def decode_to_tensor(decode_out, seq_len, device):\n",
    "    # 1) list of lists → list of 1D tensors\n",
    "    token_tensors = [torch.tensor(seq, dtype=torch.long, device=device)\n",
    "                     for seq in decode_out]\n",
    "    # 2) hiç prediction yoksa boş tensor\n",
    "    if not token_tensors:\n",
    "        return torch.zeros((0, seq_len), dtype=torch.long, device=device)\n",
    "    # 3) pad_sequence ile batch_first ve padding_value=-1\n",
    "    padded = pad_sequence(token_tensors, batch_first=True, padding_value=-1)\n",
    "    # 4) eğer hâlâ seq_len’den kısa ise sağa pad et\n",
    "    if padded.size(1) < seq_len:\n",
    "        pad_amt = seq_len - padded.size(1)\n",
    "        padded = F.pad(padded, (0, pad_amt), value=-1)\n",
    "    return padded\n",
    "\n",
    "# kullanım:\n",
    "tr_pred = decode_to_tensor(tr_decode, seq_len, device)\n",
    "it_pred = decode_to_tensor(it_decode, seq_len, device)\n",
    "\n",
    "\n",
    "# list of lists → tensor’a çevir\n",
    "if len(tr_decode) > 0:\n",
    "    tr_pred = torch.full((len(tr_decode), seq_len),\n",
    "                         fill_value=-1, dtype=torch.long, device=device)\n",
    "    for i, seq in enumerate(tr_decode):\n",
    "        tr_pred[i, :len(seq)] = torch.tensor(seq, device=device)\n",
    "else:\n",
    "    tr_pred = torch.zeros((0, seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "if len(it_decode) > 0:\n",
    "    it_pred = torch.full((len(it_decode), seq_len),\n",
    "                         fill_value=-1, dtype=torch.long, device=device)\n",
    "    for i, seq in enumerate(it_decode):\n",
    "        it_pred[i, :len(seq)] = torch.tensor(seq, device=device)\n",
    "else:\n",
    "    it_pred = torch.zeros((0, seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "# 1) Tam batch için pred tensor’ü oluştur\n",
    "all_pred = torch.full(\n",
    "    (batch_size, seq_len),\n",
    "    fill_value=-1,                # pad bölgelerde -1 kalsın\n",
    "    dtype=torch.long,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 2) TR ve IT tahminlerini ilgili index’lere yerleştir\n",
    "all_pred[tr_indices] = tr_pred\n",
    "all_pred[it_indices] = it_pred\n",
    "\n",
    "# 3) Sadece gerçek token’lar üzerinde doğru/yanlış kontrolü\n",
    "valid_mask   = labels.ne(-1)        # True olan yerler gerçek token\n",
    "correct_mask = (all_pred == labels) & valid_mask\n",
    "\n",
    "# 4) İsterseniz token‐seviye accuracy\n",
    "token_accuracy = correct_mask.sum().float() / valid_mask.sum().float()\n",
    "print(f\"Token‐seviye accuracy: {token_accuracy:.4f}\")\n",
    "\n",
    "# 5) Veya örnek‐seviye (sentence‐seviye) doğru karar:\n",
    "#    her cümlenin tüm token’ları doğru mu?\n",
    "sent_correct     = correct_mask.all(dim=1) & valid_mask.any(dim=1)\n",
    "# valid_mask.any(dim=1) ile “tamamen pad’li” örnekleri atlıyoruz\n",
    "sentence_accuracy = sent_correct.sum().float() / sent_correct.numel()\n",
    "print(f\"Sentence‐seviye accuracy: {sentence_accuracy:.4f}\")\n",
    "\n",
    "# all_pred şimdi (batch_size, seq_len) shape’inde, \n",
    "# her satır orijinal sırasıyla tahminleri içeriyor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 3, 3, 0, 0, 0, 1, 0, 2, 3, 2, 3, 2, 3, 3],\n",
       "        [0, 0, 0, 0, 1, 0, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 0],\n",
       "        [1, 3, 2, 0, 2, 0, 0, 3, 0, 2, 2, 3, 1, 2, 3, 0, 0, 0],\n",
       "        [2, 2, 0, 3, 2, 3, 0, 1, 1, 0, 3, 1, 0, 0, 0, 0, 1, 0],\n",
       "        [1, 0, 3, 0, 3, 2, 2, 3, 2, 0, 3, 3, 2, 0, 0, 1, 1, 2],\n",
       "        [0, 3, 0, 3, 0, 0, 0, 0, 1, 2, 3, 1, 3, 2, 3, 0, 3, 0],\n",
       "        [3, 0, 2, 0, 0, 0, 3, 3, 2, 3, 1, 1, 2, 2, 3, 0, 0, 3],\n",
       "        [1, 1, 0, 1, 0, 1, 3, 3, 0, 2, 3, 0, 0, 1, 0, 2, 0, 2],\n",
       "        [2, 2, 0, 3, 0, 0, 3, 0, 3, 2, 0, 3, 0, 3, 2, 3, 2, 0],\n",
       "        [3, 0, 0, 0, 3, 2, 2, 3, 2, 3, 3, 0, 3, 2, 3, 2, 0, 0],\n",
       "        [3, 1, 3, 2, 2, 3, 2, 1, 3, 0, 3, 0, 0, 1, 3, 0, 3, 2],\n",
       "        [0, 0, 1, 3, 3, 2, 2, 1, 1, 0, 3, 0, 0, 2, 3, 0, 1, 1],\n",
       "        [3, 3, 2, 3, 0, 3, 2, 0, 2, 0, 0, 0, 0, 1, 2, 3, 3, 3],\n",
       "        [0, 0, 1, 2, 2, 1, 0, 2, 3, 0, 0, 3, 0, 3, 0, 1, 3, 3],\n",
       "        [0, 2, 3, 0, 3, 3, 0, 2, 3, 0, 2, 0, 0, 0, 3, 3, 2, 0],\n",
       "        [2, 0, 0, 3, 2, 2, 3, 1, 3, 2, 3, 3, 0, 0, 0, 3, 3, 0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
